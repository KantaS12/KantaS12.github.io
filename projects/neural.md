---
layout: project
type: project
image: img/vacay/vacay-square.png
title: "Vacay"
date: 2015
published: true
labels:
  - Python
  - NumPy
  - MNIST Dataset
  - Matplotlib
summary: "Implementation of a basic feedforward neural network from scratch using Python and the NumPy library."
---

<img class="img-fluid" src="../img/vacay/vacay-home-page.png">

For this project, I embarked on the challenge of constructing a neural network from scratch to predict handwritten digits from the MNIST dataset. 

A pivotal moment in my learning came from grasping the concept of partial derivatives, an essential element for implementing backpropagation. I thoroughly explored the impact of five distinct activation functions: ReLU, Softmax, SeLU, Tanh, and Sigmoid, which proved instrumental in understanding their individual contributions to the network's performance. The DeepLearning.AI machine learning course further enhanced my comprehension of ReLU, Softmax, and Sigmoid. 

Ultimately, this project provided an immensely rewarding experience in unraveling the complexities of neural networks and the nuanced roles of activation functions

Here is some example code to illustrate Simple Schema use:

{% gist 9defa1fb3f4eb593ba5fa9eacedca960 %}
 
Source: <a href="https://github.com/theVacay/vacay">theVacay/vacay</a>
